{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Etapa de entrenamiento y testeo de un modelo de análisis de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, TreebankWordTokenizer, RegexpTokenizer\n",
    "from nltk import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from time import time\n",
    "from stop_words import get_stop_words\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split # Modelado\n",
    "from sklearn.pipeline import Pipeline # Modelado\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Modelado\n",
    "from sklearn.feature_selection import chi2 # Reporte\n",
    "from sklearn.linear_model import LogisticRegression # Reporte\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve # Reporte\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los datos\n",
    "path = r'C:\\Users\\CARLES\\1.CARLES\\FORMACIONES\\BOOTCAMP KEEPCODING\\10. NLP'\n",
    "\n",
    "clean_df = pd.read_csv(f'{path}\\\\reduced_df_clean_nlp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separamos en conjunto de train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los conjuntos de entrenamiento (75% del total) y test (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como mas adelante vemos que los Nan estan dando error, los vamos a eliminar\n",
    "clean_df = clean_df.dropna(subset=['processedReview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    clean_df['processedReview'],\n",
    "    clean_df['is_negative_sentiment'],\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "X_train.to_csv(r'C:\\Users\\CARLES\\1.CARLES\\FORMACIONES\\BOOTCAMP KEEPCODING\\10. NLP\\X_train.csv', index = False)\n",
    "X_test.to_csv(r'C:\\Users\\CARLES\\1.CARLES\\FORMACIONES\\BOOTCAMP KEEPCODING\\10. NLP\\X_test.csv', index = False)\n",
    "y_train.to_csv(r'C:\\Users\\CARLES\\1.CARLES\\FORMACIONES\\BOOTCAMP KEEPCODING\\10. NLP\\y_train.csv', index = False)\n",
    "y_test.to_csv(r'C:\\Users\\CARLES\\1.CARLES\\FORMACIONES\\BOOTCAMP KEEPCODING\\10. NLP\\y_test.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343241                                   like fast delivery\n",
       "145064             worth rip hair dollar store quality hair\n",
       "299900                                 great fun play price\n",
       "460812                    great year old mickey mouse party\n",
       "280141    bought pen tried different one loved concept w...\n",
       "64064     product seems dry coming leave clean smooth li...\n",
       "459291                                nice sturdy wife love\n",
       "353847    excellent dry use applying la prairie serum ce...\n",
       "411650    lightweight simply designed end curved allow u...\n",
       "38306     good epilator still pas multiple time get ever...\n",
       "Name: processedReview, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343241    0\n",
       "145064    1\n",
       "299900    0\n",
       "460812    0\n",
       "280141    0\n",
       "64064     1\n",
       "459291    0\n",
       "353847    0\n",
       "411650    0\n",
       "38306     0\n",
       "Name: is_negative_sentiment, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cv_tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=3,\n",
    "    max_features=10000,\n",
    "    strip_accents='ascii',\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "cv.fit(X_train)\n",
    "\n",
    "# Nombre de archivo donde se guardará el TfidfVectorizer\n",
    "archivo_cv = 'cv_tfidf_vectorizer.pkl'\n",
    "\n",
    "# Guardar el TfidfVectorizer\n",
    "joblib.dump(cv, archivo_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El TfidfVectorizer se configura con max_df=0.95 y min_df=3 para eliminar términos muy comunes y muy raros, reduciendo ruido y dimensionalidad. \n",
    "Se limitan las características a 10,000 palabras (max_features=10000) para mantener un buen equilibrio entre información y manejabilidad. \n",
    "La opción strip_accents='ascii' normaliza el texto eliminando acentos. \n",
    "ngram_range=(1, 1) se usa para enfocarse en unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 4926), ('fast', 3237), ('delivery', 2227), ('worth', 9901), ('rip', 7303), ('hair', 3968), ('dollar', 2564), ('store', 8425), ('quality', 6787), ('great', 3892), ('fun', 3637), ('play', 6383), ('price', 6600), ('year', 9948), ('old', 5885), ('mickey', 5384), ('mouse', 5566), ('party', 6122), ('bought', 955), ('pen', 6193)]\n"
     ]
    }
   ],
   "source": [
    "print(list(cv.vocabulary_.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de Características de Texto con TF-IDF\n",
    "X_train_ = cv.transform(X_train)\n",
    "X_test_ = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7986858931215507\n",
      "Accuracy for C=0.05: 0.8287033921119935\n",
      "Accuracy for C=0.25: 0.8383783147126127\n",
      "Accuracy for C=0.5: 0.8398758244716651\n",
      "Accuracy for C=1: 0.8401113878045497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CARLES\\anaconda3\\envs\\nlp_bootcamp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=10: 0.8383951406649617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CARLES\\anaconda3\\envs\\nlp_bootcamp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=100: 0.8367546103109436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CARLES\\anaconda3\\envs\\nlp_bootcamp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=1000: 0.8364349172163145\n",
      "Accuracy for C=10000: 0.8366957194777225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CARLES\\anaconda3\\envs\\nlp_bootcamp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test_acc_lr.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_params = [0.01, 0.05, 0.25, 0.5, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "train_acc = list()\n",
    "test_acc = list()\n",
    "for c in c_params:\n",
    "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=500)\n",
    "    lr.fit(X_train_, y_train)\n",
    "\n",
    "    train_predict = lr.predict(X_train_)\n",
    "    test_predict = lr.predict(X_test_)\n",
    "\n",
    "    print (\"Accuracy for C={}: {}\".format(c, accuracy_score(y_test, test_predict)))\n",
    "\n",
    "    train_acc.append(accuracy_score(y_train, train_predict))\n",
    "    test_acc.append(accuracy_score(y_test, test_predict))\n",
    "\n",
    "# Nombre de archivo donde se guardará el modelo\n",
    "archivo_c_params = 'c_params.pkl'\n",
    "archivo_modelo = 'logistic_regression_model_nlp.pkl'\n",
    "archivo_predicciones = 'predicciones_test_lr.pkl'\n",
    "archivo_train_acc = 'train_acc_lr.pkl'\n",
    "archivo_test_acc = 'test_acc_lr.pkl'\n",
    "\n",
    "# Guardar el modelo y las predicciones\n",
    "joblib.dump(c_params, archivo_c_params)\n",
    "joblib.dump(lr, archivo_modelo)\n",
    "joblib.dump(test_predict, archivo_predicciones)\n",
    "joblib.dump(train_acc, archivo_train_acc)\n",
    "joblib.dump(test_acc, archivo_test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confussion matrix:\n",
      "[[75188  7726]\n",
      " [11685 24265]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89     82914\n",
      "           1       0.76      0.67      0.71     35950\n",
      "\n",
      "    accuracy                           0.84    118864\n",
      "   macro avg       0.81      0.79      0.80    118864\n",
      "weighted avg       0.83      0.84      0.83    118864\n",
      "\n",
      "Accuracy score:0.8366957194777225\n"
     ]
    }
   ],
   "source": [
    "# Metricas sobre los resultados\n",
    "\n",
    "print('Confussion matrix:\\n{}'.format(confusion_matrix(y_test, test_predict)))\n",
    "print('\\nClassification report:\\n{}'.format(classification_report(y_test, test_predict)))\n",
    "print('Accuracy score:{}'.format(accuracy_score(y_test, test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDES NEURONALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X = clean_df['processedReview']\n",
    "y = clean_df['is_negative_sentiment']\n",
    "\n",
    "max_words = 1000  # Número máximo de palabras a considerar\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Padding para hacer todas las secuencias del mismo tamaño\n",
    "maxlen = 20  # Longitud máxima de las secuencias\n",
    "X_padded = pad_sequences(X_sequences, maxlen=maxlen)\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 16)            16000     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               46800     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62901 (245.71 KB)\n",
      "Trainable params: 62901 (245.71 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# Crear el modelo de red neuronal\n",
    "model = Sequential()\n",
    "\n",
    "# Capa de Embedding\n",
    "embedding_dim = 16\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen))\n",
    "\n",
    "# Capa LSTM\n",
    "model.add(LSTM(units=100))\n",
    "\n",
    "# Capas densas\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Salida lineal para regresión\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Resumen del modelo\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5943/5943 [==============================] - 71s 12ms/step - loss: 0.3999 - accuracy: 0.8126 - precision_1: 0.7329 - recall_1: 0.5971 - val_loss: 0.3633 - val_accuracy: 0.7969 - val_precision_1: 0.6875 - val_recall_1: 0.5789\n",
      "Epoch 2/3\n",
      "5943/5943 [==============================] - 63s 11ms/step - loss: 0.3792 - accuracy: 0.8232 - precision_1: 0.7449 - recall_1: 0.6307 - val_loss: 0.3421 - val_accuracy: 0.8594 - val_precision_1: 0.7500 - val_recall_1: 0.7895\n",
      "Epoch 3/3\n",
      "5943/5943 [==============================] - 63s 11ms/step - loss: 0.3729 - accuracy: 0.8265 - precision_1: 0.7504 - recall_1: 0.6378 - val_loss: 0.3338 - val_accuracy: 0.8281 - val_precision_1: 0.7222 - val_recall_1: 0.6842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a4d818fdc0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]  # first batch_size samples\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]  # rest for training\n",
    "\n",
    "model.fit(X_train2, y_train2,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3746207058429718\n",
      "accuracy: 0.8264924883842468\n",
      "precision_1: 0.7607714533805847\n",
      "recall_1: 0.621552050113678\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)  # Devuelve la pérdida y otras métricas especificadas en model.compile()\n",
    "\n",
    "# Obtener los nombres de las métricas\n",
    "metrics_names = model.metrics_names\n",
    "\n",
    "# Imprimir cada métrica con su nombre correspondiente\n",
    "for name, score in zip(metrics_names, scores):\n",
    "    print(f\"{name}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el modelo de Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Definir el directorio de almacenamiento del modelo\n",
    "cache_dir = './models'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "model_file = \"lstm_model_nlp.keras\"  # Formato nativo de Keras\n",
    "\n",
    "# Guardar el modelo en el formato nativo de Keras\n",
    "model.save(os.path.join(cache_dir, model_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
